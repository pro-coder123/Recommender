{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from html2text import html2text\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Job URL Function\n",
    "def extract_job_urls_from_result(soup): \n",
    "    urls = []\n",
    "    for div in soup.find_all(name = \"div\", attrs = {\"class\":\"row\"}):\n",
    "        for a in div.find_all(name = \"a\", attrs = {\"data-tn-element\":\"jobTitle\"}):\n",
    "            this_url = a['href']\n",
    "            to_go_url = \"https://www.indeed.com/viewjob\" + this_url[7:]\n",
    "            urls.append(to_go_url)\n",
    "    return(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Job Title & Description\n",
    "def extract_text_from_jobURL(url):\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    #Retrive the job title\n",
    "    b = soup.find('b','jobtitle')\n",
    "    if b != None:        \n",
    "        #Retrive the job description\n",
    "        table = soup.find('table', id = 'job-content')\n",
    "        span = table.find('span', id = 'job_summary')\n",
    "\n",
    "        div = span.find('div')\n",
    "\n",
    "        #Because in some posts there isn't exist the div class\n",
    "        if div==None:\n",
    "            div = span\n",
    "\n",
    "        #Cleaning (remove newllines, * , spaces)\n",
    "        title = html2text(str(b)).strip().replace('*', '')\n",
    "        #clean = html2text(str(div)).strip().replace('*','').replace('#', '')\n",
    "        clean = ' '.join(html2text(str(div)).replace('*','').replace('#', '').split())\n",
    "    \n",
    "        return(title, clean)\n",
    "    else:\n",
    "        return([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Job Title Dataset\n",
    "job_titles = pd.read_csv(\"..\\..\\Datasets\\Job_Titles.csv\", sep = \",\")\n",
    "\n",
    "#Create job_titles list\n",
    "job_list = list(job_titles.Job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Listings URLs - a list of (query_job, url) tuples\n",
    "\n",
    "#Initialize the url job list and city list\n",
    "listing_urls_list = []\n",
    "city_list = ['New+York', 'London']\n",
    "\n",
    "for city in city_list:\n",
    "    for job in job_list:\n",
    "        #for start in range(0,20,10):\n",
    "            #URL=\"http://www.indeed.com/jobs?q=\"+ job.replace(' ', '+') + \"%2420%2C000&l=\" + str(city) + \"&start=\" + str(start)\n",
    "        URL=\"http://www.indeed.com/jobs?q=\"+ job.replace(' ', '+') + \"%2420%2C000&l=\" + str(city) + \"&limit=20\"\n",
    "        listing_urls_list.append((job, URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f948178bc5425f9727d28493deb7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Loading Jobs', max=1500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Main loop\n",
    "\n",
    "#Initialize the dataframe\n",
    "columns = [\"ID\", \"Query\", \"Job Title\", \"Description\"]\n",
    "df = pd.DataFrame(columns = columns)\n",
    "\n",
    "n = 0\n",
    "max_items = 1500 #30000 to get all\n",
    "\n",
    "#Visited urls set\n",
    "visited = set()\n",
    "check_visited = True\n",
    "\n",
    "#Jupyter Progress Bar init\n",
    "pbar = tqdm_notebook(total = max_items, desc = 'Loading Jobs')\n",
    "\n",
    "#Outer loop - go over the query results (many jobs in each)\n",
    "for query_job, url in listing_urls_list:\n",
    "\n",
    "    if n != max_items:\n",
    "        if (check_visited and url in visited):\n",
    "            continue\n",
    "        \n",
    "        #Conducting a request of the stated Job Listings URL above:\n",
    "        page = requests.get(url)\n",
    "\n",
    "        #Specifying a desired format of “page” using the html parser\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "        #Retrieve the specific job urls from this listing\n",
    "        job_urls_found = extract_job_urls_from_result(soup)\n",
    "\n",
    "        #Check if there is a job in indeed\n",
    "        if job_urls_found != []: #found some jobs for this query\n",
    "            \n",
    "            #Inner loop - retrieve the title and the description for each job found\n",
    "            for item in job_urls_found:\n",
    "\n",
    "                #Extract job info (title and description)\n",
    "                job_info = extract_text_from_jobURL(item)\n",
    "\n",
    "                #Add a slight delay to avoid getting blocked\n",
    "                #time.sleep(0.2)\n",
    "\n",
    "                #Add it as a new row to the dataframe\n",
    "                if job_info != []:\n",
    "                    n += 1\n",
    "                    if (check_visited):\n",
    "                        visited.add(url)\n",
    "                    pbar.update(1) #update progress bar\n",
    "                    df.loc[len(df)] = [n, query_job, job_info[0], job_info[1]]\n",
    "\n",
    "                if n == max_items:\n",
    "                    pbar.close()\n",
    "                    break\n",
    "    else: #all downloaded, stop\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a csv file\n",
    "df.to_csv(\"../Datasets/results.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
