{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from html2text import html2text\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Job URLs\n",
    "\"\"\"\n",
    "Input: soup object containing the result of a job query (e.g. Data Scientist jobs in New York)\n",
    "Output: list of the individual job urls of the jobs found in the query\n",
    "\"\"\"\n",
    "def extract_job_urls_from_result(query_soup): \n",
    "    urls = []\n",
    "    for div in query_soup.find_all(name = \"div\", attrs = {\"class\":\"row\"}):\n",
    "        for a in div.find_all(name = \"a\", attrs = {\"data-tn-element\":\"jobTitle\"}):\n",
    "            this_url = a['href']\n",
    "            to_go_url = \"https://www.indeed.com/viewjob\" + this_url[7:]\n",
    "            urls.append(to_go_url)\n",
    "    return(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Job Title & Description\n",
    "\"\"\"\n",
    "Input: The full URL of a job that will be scraped\n",
    "Output: A tuple containing the job title & the description, after applying some basic cleaning\n",
    "        On error, returns None.\n",
    "\"\"\"\n",
    "def extract_text_from_jobURL(url):\n",
    "    #Request the job url and get a soup object reference to its contents\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    \"\"\"There are two different methods Indeed.com uses to list its jobs, so we\n",
    "    need to adapt. To check which is used, we will first get the job title. \"\"\"\n",
    "    \n",
    "    #Method One - the job title is in a <b> tag\n",
    "    title = soup.find('b', 'jobtitle')\n",
    "    if title != None:\n",
    "        \n",
    "        #Retrieve the job description\n",
    "        table = soup.find('table', id = 'job-content')\n",
    "        span = table.find('span', id = 'job_summary')\n",
    "\n",
    "        description = span.find('div') #Usually found inside this span's div\n",
    "        if description == None: #But in some posts there doesn't exist the div class\n",
    "            description = span\n",
    "    \n",
    "    #Method Two - the job title is in a <h3> tag\n",
    "    else:\n",
    "        title = soup.find('h3', 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title')\n",
    "        if title != None:\n",
    "            \n",
    "            #Retrieve the job description\n",
    "            description = soup.find('div', 'jobsearch-JobComponent-description icl-u-xs-mt--md')\n",
    "    \n",
    "    #Checking complete, now return the result after applying basic cleaning to the description.\n",
    "    if title is not None:\n",
    "        title = html2text(str(title)).strip().replace('*', '').replace('#', '')\n",
    "        clean_desc = ' '.join(html2text(str(description)).replace('*','').replace('#', '').split())\n",
    "\n",
    "        return(title, clean_desc)\n",
    "    else:\n",
    "        #print(\"Other HTML format was used! >:| \")\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_path = \"..\\..\\Datasets\\job_titles_IT.csv\"\n",
    "results_path = \"..\\..\\Results\\TempResults.csv\"\n",
    "urls_path = \"..\\..\\Results\\setURLs.csv\"\n",
    "\n",
    "#job_titles_path = \"..\\Datasets\\Best Jobs in America.csv\" #source: https://money.cnn.com/pf/best-jobs/2017/list/index.html\n",
    "\n",
    "city_list = ['New+York', 'Los+Angeles', 'Chicago', 'Houston',\n",
    "             'Washington', 'Dallas', 'Seattle', 'Silicon+Valley',\n",
    "             'Detroit', 'San+Francisco', 'Austin', 'Philadelphia',\n",
    "             'Boston', 'Minneapolis', 'Phoenix', 'San+Jose']\n",
    "\n",
    "max_jobs_to_get = None #optional, used to set total # of jobs to download, else leave None\n",
    "jobs_perQuery_perCity = 25 #must be <= 50, take into account # of inaccessible ads\n",
    "\n",
    "jobs_stored = 9800 #total number of jobs stored in the .csv\n",
    "queries_completed = jobs_stored // jobs_perQuery_perCity #used to resume downloading\n",
    "append_mode = True #control whether the .csv exists and should the results be appended\n",
    "\n",
    "checkpoint_interval = 100 #how often to store the results into the .csv\n",
    "allow_duplicates = False #checks for duplicate jobs (over each program run only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Job Titles Data\n",
    "job_titles = pd.read_csv(job_titles_path, sep = \",\")\n",
    "\n",
    "#Create job titles list\n",
    "job_list = list(job_titles.Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create queries URLs - a list of (query_job, query_url) tuples\n",
    "queries_list = []\n",
    "\n",
    "for query_job in job_list:\n",
    "    for city in city_list:\n",
    "#        for start in range(0, 150, 50):\n",
    "        query_url = \"http://www.indeed.com/jobs?q=\" + query_job.replace(' ', '+') + \\\n",
    "                    \"%2420%2C000&l=\" + str(city) + \\\n",
    "                    \"&limit=50\" #+ \"&start=\" + str(start)\n",
    "        queries_list.append((query_job, query_url))\n",
    "\n",
    "#Initial Setup\n",
    "df = pd.DataFrame(columns = [\"ID\", \"Query\", \"Job Title\", \"Description\"])\n",
    "\n",
    "\n",
    "if max_jobs_to_get is None: #set number of jobs to be downloaded (in the end result)\n",
    "    max_jobs_to_get = jobs_perQuery_perCity * len(queries_list)\n",
    "print(\"max_jobs_to_get :\", max_jobs_to_get)\n",
    "\n",
    "if (not allow_duplicates):\n",
    "    if jobs_stored == 0: #no URLs stored either\n",
    "        urls_df = pd.DataFrame(columns = [\"ID\", \"URL\"])\n",
    "        visited = set()\n",
    "        urls_df.to_csv(urls_path, index = False)\n",
    "    else: #load the previously stored jobs' URLs (to avoid re-storing them)\n",
    "        urls_df = pd.read_csv(urls_path, sep = \",\")\n",
    "        visited = set(list(urls_df.URL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Jupyter Progress Bar init\n",
    "pbar = tqdm_notebook(initial = jobs_stored, total = max_jobs_to_get, desc = 'Loading Jobs')\n",
    "\n",
    "#Outer loop - go over the query results (many jobs in each)\n",
    "for query_job, query_url in queries_list[queries_completed:]:\n",
    "\n",
    "    if jobs_stored != max_jobs_to_get:\n",
    "        #Send a request over the query URL and get a BeautifulSoup object out of it:\n",
    "        try:\n",
    "            page = requests.get(query_url, timeout = 300)\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        except OSError:\n",
    "            print(\"OSError was caught at query retrieval! Saving and exiting...\")\n",
    "            if (not append_mode): #first time saving\n",
    "                df.to_csv(results_path, index = False)\n",
    "                append_mode = True\n",
    "            else:\n",
    "                df.to_csv(results_path, mode = 'a', index = False, header = False)\n",
    "            break\n",
    "\n",
    "        #Retrieve the specific job urls from this listing\n",
    "        job_urls = extract_job_urls_from_result(soup)\n",
    "\n",
    "        #Check if there is a job in indeed\n",
    "        if job_urls != []: #found some jobs for this query\n",
    "            \n",
    "            query_jobs_stored = 0 #the jobs found for this specific query\n",
    "            \n",
    "            #Inner loop - retrieve the title and the description for each job found\n",
    "            for item in job_urls:\n",
    "                \n",
    "                if query_jobs_stored == jobs_perQuery_perCity: #collected enough jobs\n",
    "                    break\n",
    "                \n",
    "                if item in visited: #already captured, ignore\n",
    "                    continue\n",
    "\n",
    "                #Extract job info (title and description)\n",
    "                try:\n",
    "                    job_info = extract_text_from_jobURL(item)\n",
    "                    #time.sleep(0.2) #Add a slight delay to avoid getting blocked\n",
    "                except OSError: #network error or timeout\n",
    "                    print(\"OSError was caught and ignored!\")\n",
    "                    continue\n",
    "\n",
    "                if job_info is not None: #all good\n",
    "                    query_jobs_stored += 1\n",
    "                    jobs_stored += 1\n",
    "\n",
    "                    #store the job details in the dataframe\n",
    "                    df.loc[len(df)] = [jobs_stored, query_job, job_info[0], job_info[1]]\n",
    "                    \n",
    "                    if (not allow_duplicates):\n",
    "                        visited.add(item) #mark it as visited so to not store it again\n",
    "                        urls_df.loc[len(urls_df)] = [jobs_stored, item]\n",
    "                    \n",
    "                    pbar.update(1) #update progress bar\n",
    "                else:\n",
    "                    print(\"Job retrieval failed for url:\", item)\n",
    "                    continue\n",
    "                    \n",
    "                if ((jobs_stored % checkpoint_interval) == 0): #if it's time to save\n",
    "                    print(\"Storing the results...Total saved up to now:\", jobs_stored)\n",
    "                    if (not append_mode): #first time saving\n",
    "                        df.to_csv(results_path, index = False)\n",
    "                        append_mode = True\n",
    "                    else:\n",
    "                        df.to_csv(results_path, mode = 'a', index = False, header = False)\n",
    "                    df = df.iloc[0:0] #empty contents in memory\n",
    "                    if (not allow_duplicates):\n",
    "                        urls_df.to_csv(urls_path, mode = 'a', index = False, header = False)\n",
    "                        urls_df = urls_df.iloc[0:0]\n",
    "                        \n",
    "    else: #all downloaded, stop\n",
    "        pbar.close()\n",
    "        break\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
