{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geop\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from six.moves import urllib\n",
    "# import from nltk the functions that split a text into sentences and tokens\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pprint import pprint\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, chunk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the entire dataset\n",
    "data = pd.read_csv('../../Results/JobsDataset.csv', header = 0, names = ['Query', 'Job Title', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Job description list\n",
    "job_descriptions=[]\n",
    "for job in data.Description:\n",
    "    j = job.replace(',', '')\n",
    "    job_descriptions.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words tokenization\n",
    "jobs = [word_tokenize(d) for d in job_descriptions]\n",
    "\n",
    "#Remove Capitalization\n",
    "no_capitals =[]\n",
    "for job in jobs:\n",
    "    no_capitals.append([j.lower() for j in job])\n",
    "\n",
    "#Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem=[]\n",
    "for job in no_capitals:\n",
    "    lem.append([lemmatizer.lemmatize(j) for j in job])\n",
    "\n",
    "#Remove stopwords\n",
    "filtered_words = []\n",
    "for job in lem:\n",
    "    filtered_words.append([j for j in job if not j in stopwords.words('english')])\n",
    "\n",
    "#Remove symbols\n",
    "cleaned_description=[]\n",
    "for job in filtered_words:\n",
    "    cleaned_description.append([j for j in job if not j in ['(',')','.',',',':','%']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "model = Word2Vec(cleaned_description , size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python [('scala', 0.8651583194732666), ('c++', 0.8401311635971069), ('matlab', 0.8297944664955139), ('java', 0.8285940885543823), ('ruby', 0.8285742402076721), ('perl', 0.7943324446678162), ('numpy', 0.7938722372055054), ('php', 0.7857421636581421), ('scripting', 0.7811324596405029), ('bash', 0.7795597910881042)]\n",
      "\n",
      "\n",
      "java [('javascript', 0.8764125108718872), ('php', 0.8751732110977173), ('c++', 0.8750032186508179), ('scala', 0.8684266209602356), ('node.js', 0.8542532920837402), ('.net', 0.8515223860740662), ('c/c++', 0.8349071145057678), ('nodejs', 0.8311883211135864), ('python', 0.8285940885543823), ('ruby', 0.8256882429122925)]\n",
      "\n",
      "\n",
      "data [('datasets', 0.6203596591949463), ('metadata', 0.5921299457550049), ('output', 0.5661672949790955), ('predictive', 0.5607515573501587), ('structured', 0.5573886632919312), ('extract', 0.5462326407432556), ('etl', 0.5440762042999268), ('cleansing', 0.5416525602340698), ('reporting', 0.5336347222328186), ('database', 0.505217969417572)]\n",
      "\n",
      "\n",
      "c [('ee', 0.7746924757957458), ('asp.net', 0.7702680826187134), ('c++', 0.7425520420074463), ('vb.net', 0.7311525344848633), ('j', 0.7103360891342163), ('c/c++', 0.6991377472877502), ('laravel', 0.6963754296302795), ('perl', 0.6959887146949768), ('wpf', 0.6949141025543213), ('rdl.net', 0.6906944513320923)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Skills Similarity\n",
    "for seed_word in [ 'python', 'java', 'data', 'c']:\n",
    "    print(seed_word,model.wv.most_similar(positive=[seed_word], topn=10))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
