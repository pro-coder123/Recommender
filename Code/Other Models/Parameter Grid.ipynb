{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; width: 90%; height: auto; line-height: 200px; vertical-align: middle; background: #000099; padding: 0; margin: 0; border-top-left-radius: 10px; border-bottom-left-radius: 10px; border-top-right-radius: 10px; border-bottom-right-radius: 10px; font-family: Calibri; font-size: 50px; overflow: hidden; text-align: center;\">\n",
    "        <div style=\"text-align: center; width: 80%; margin: 0 auto; overflow: hidden; line-height: 80px; background-color:#000099\">\n",
    "                  <span id=\"dept_name\" class=\"pretty_span\">\n",
    "                          <font color=\"white\">**Parameter Grid**</font>\n",
    "                  </span>\n",
    "        </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; width: 90%; height: auto; line-height: 200px; vertical-align: middle; background: #000000; padding: 0; margin: 0; border-top-left-radius: 10px; border-bottom-left-radius: 10px; border-top-right-radius: 10px; border-bottom-right-radius: 10px; font-family: Calibri; font-size: 30px; overflow: hidden; text-align: center;\">\n",
    "        <div style=\"text-align: center; width: 80%; margin: 0 auto; overflow: hidden; line-height: 50px; background-color:#000000\">\n",
    "                  <span id=\"dept_name\" class=\"pretty_span\">\n",
    "                          <font color=\"white\">**Table of contents**</font>\n",
    "                  </span>\n",
    "        </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [Import Libraries](#Import Libraries)\n",
    "## 2. [Load the Dataset](#Load the Dataset)\n",
    "## 3. [Create the Train and Test Datasets](#Create the Train and Test Datasets)\n",
    "## 4. [Define Tokenizer with Vocab Size](#Define Tokenizer with Vocab Size)\n",
    "## 5. [Create the Model](#Create the Model)\n",
    "## 6. [Reproductibility and Model Definition](#Reproductibility and Model Definition)\n",
    "## 7. [Create and Tune the Parameter Grip](#Create and Tune the Parameter Grip)\n",
    "## 8. [Get the results of all Models and Find the Best One](#Get the results of all Models and Find the Best One)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Import Libraries'></a> <u><font color='orange'>Import Libraries</font></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import metrics\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, chunk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Dropout\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Load the Dataset'></a> <u><font color='orange'>Load the Dataset</font></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/leonidas/Documents/GitHub/Deep-Learning-Project/Results/'\n",
    "filename = '25_cleaned_job_descriptions.csv'\n",
    "data = pd.read_csv(os.path.join(path, filename), header = 0, names = ['Query', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Create the Train and Test Datasets'></a> <u><font color='orange'>Create the Train and Test Datasets</font></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to train and test (80 - 20)\n",
    "\n",
    "train, test = train_test_split(data, test_size = 0.2)\n",
    "\n",
    "train_descs = train['Description']\n",
    "train_labels = train['Query']\n",
    "#train_labels = train['Job Title']\n",
    " \n",
    "test_descs = test['Description']\n",
    "test_labels = test['Query']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Define Tokenizer with Vocab Size'></a> <u><font color='orange'>Define Tokenizer with Vocab Size</font></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(train_labels.unique().tolist())\n",
    "vocab_size = 1000\n",
    " \n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_descs)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_descs, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_descs, mode='tfidf')\n",
    " \n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Create the Model'></a> <u><font color='orange'>Create the Model</font></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', init_mode='uniform', activation='relu', dropout_rate=0.0, weight_constraint=0, neurons=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(vocab_size,), activation = activation, kernel_initializer = init_mode, kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(512, kernel_initializer = init_mode, activation= activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = optimizer, # or 'sgd'\n",
    "              metrics = [metrics.categorical_accuracy, 'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Reproductibility and Model Definition'></a> <u><font color='orange'>Reproductibility and Model Definition</font></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define the model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Create and Tune the Parameter Grip'></a> <u><font color='orange'>Create and Tune the Parameter Grip</font></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "epochs = [30, 50, 100]\n",
    "batch_size = [16, 32, 64, 100, 200]\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "neurons = [512, 1024, 2048, 4096]\n",
    "param_grid = dict(optimizer=optimizer, init_mode=init_mode, epochs=epochs, batch_size=batch_size, activation=activation, dropout_rate=dropout_rate, weight_constraint=weight_constraint, neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Get the results of all Models and Find the Best One'></a> <u><font color='orange'>Get the results of all Models and Find the Best One</font></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
